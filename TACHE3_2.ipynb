{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports et Configuration\n",
    "Import des bibliothèques nécessaires (transformers, torch) et configuration initiale des modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Imports et Configuration\n",
    "\n",
    "# Import des bibliothèques nécessaires\n",
    "import torch\n",
    "from transformers import pipeline, CamembertTokenizer, CamembertForTokenClassification\n",
    "\n",
    "# Configuration initiale des modèles\n",
    "# Modèle pour l'analyse grammaticale\n",
    "pos_model_name = \"qanastek/pos-french-camembert\"\n",
    "pos_tokenizer = CamembertTokenizer.from_pretrained(pos_model_name)\n",
    "pos_model = CamembertForTokenClassification.from_pretrained(pos_model_name)\n",
    "pos_pipeline = pipeline(\"ner\", model=pos_model, tokenizer=pos_tokenizer)\n",
    "\n",
    "# Modèle pour choisir le mot à insérer\n",
    "fill_mask_model_name = \"camembert-base\"\n",
    "fill_mask_pipeline = pipeline(\"fill-mask\", model=fill_mask_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des Modèles Transformers\n",
    "Chargement du modèle qanastek/pos-french pour l'analyse grammaticale et d'un modèle de langue (camembert-base) pour la sélection des mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Chargement des Modèles Transformers\n",
    "\n",
    "# Chargement du modèle qanastek/pos-french pour l'analyse grammaticale\n",
    "pos_model_name = \"qanastek/pos-french-camembert\"\n",
    "pos_tokenizer = CamembertTokenizer.from_pretrained(pos_model_name)\n",
    "pos_model = CamembertForTokenClassification.from_pretrained(pos_model_name)\n",
    "pos_pipeline = pipeline(\"ner\", model=pos_model, tokenizer=pos_tokenizer)\n",
    "\n",
    "# Chargement du modèle camembert-base pour la sélection des mots\n",
    "fill_mask_model_name = \"camembert-base\"\n",
    "fill_mask_pipeline = pipeline(\"fill-mask\", model=fill_mask_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction de Détection des Verbes\n",
    "Implémentation de la fonction qui utilise le modèle POS pour identifier les verbes dans les proverbes modifiés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁a', '▁mentir', '▁part']\n"
     ]
    }
   ],
   "source": [
    "def detect_verbs(proverb):\n",
    "    \"\"\"\n",
    "    Utilise le modèle POS pour identifier les verbes dans un proverbe modifié.\n",
    "    \n",
    "    Args:\n",
    "    proverb (str): Le proverbe modifié.\n",
    "    \n",
    "    Returns:\n",
    "    List[str]: Liste des verbes détectés dans le proverbe.\n",
    "    \"\"\"\n",
    "    # Utilisation du pipeline POS pour obtenir les étiquettes grammaticales\n",
    "    pos_tags = pos_pipeline(proverb)\n",
    "    \n",
    "    # Extraction des verbes à partir des étiquettes POS\n",
    "    verbs = [word['word'] for word in pos_tags if word['entity'] == 'VERB']\n",
    "    \n",
    "    return verbs\n",
    "\n",
    "# Exemple d'utilisation de la fonction\n",
    "proverb_example = \"a beau mentir qui part de loin\"\n",
    "detected_verbs = detect_verbs(proverb_example)\n",
    "print(detected_verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction de Sélection du Meilleur Mot\n",
    "Développement de la fonction qui utilise le modèle de langue pour choisir le meilleur mot parmi les candidats en fonction du contexte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "PipelineException",
     "evalue": "No mask_token (<mask>) found on the input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPipelineException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m proverb_example \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma beau [MASK] qui part de loin\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m word_candidates \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvient\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevient\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 30\u001b[0m best_word \u001b[38;5;241m=\u001b[39m \u001b[43mselect_best_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproverb_example\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_word)\n",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m, in \u001b[0;36mselect_best_word\u001b[0;34m(proverb, word_list)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_list:\n\u001b[1;32m     15\u001b[0m     masked_proverb \u001b[38;5;241m=\u001b[39m proverb\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[MASK]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[MASK]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mfill_mask_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_proverb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m prediction:\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_str\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m word:\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/pipelines/fill_mask.py:270\u001b[0m, in \u001b[0;36mFillMaskPipeline.__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    249\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m    Fill the masked token in the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m        - **token_str** (`str`) -- The predicted token (to replace the masked one).\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/pipelines/base.py:1302\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         )\n\u001b[1;32m   1300\u001b[0m     )\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/pipelines/base.py:1308\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m-> 1308\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1309\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1310\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/pipelines/fill_mask.py:123\u001b[0m, in \u001b[0;36mFillMaskPipeline.preprocess\u001b[0;34m(self, inputs, return_tensors, tokenizer_kwargs, **preprocess_parameters)\u001b[0m\n\u001b[1;32m    120\u001b[0m     tokenizer_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(inputs, return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs)\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mensure_exactly_one_mask_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_inputs\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/pipelines/fill_mask.py:112\u001b[0m, in \u001b[0;36mFillMaskPipeline.ensure_exactly_one_mask_token\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m input_ids \u001b[38;5;129;01min\u001b[39;00m model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 112\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_exactly_one_mask_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/pipelines/fill_mask.py:100\u001b[0m, in \u001b[0;36mFillMaskPipeline._ensure_exactly_one_mask_token\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m     98\u001b[0m numel \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mprod(masked_index\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numel \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineException(\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfill-mask\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbase_model_prefix,\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo mask_token (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mmask_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) found on the input\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    104\u001b[0m     )\n",
      "\u001b[0;31mPipelineException\u001b[0m: No mask_token (<mask>) found on the input"
     ]
    }
   ],
   "source": [
    "def select_best_word(proverb, word_list):\n",
    "    \"\"\"\n",
    "    Utilise le modèle de langue pour choisir le meilleur mot parmi les candidats en fonction du contexte du proverbe.\n",
    "    \n",
    "    Args:\n",
    "    proverb (str): Le proverbe modifié.\n",
    "    word_list (List[str]): Liste des mots candidats.\n",
    "    \n",
    "    Returns:\n",
    "    str: Le meilleur mot choisi en fonction du contexte.\n",
    "    \"\"\"\n",
    "    # Génération des prédictions pour chaque mot candidat\n",
    "    predictions = []\n",
    "    for word in word_list:\n",
    "        masked_proverb = proverb.replace(\"[MASK]\", \"[MASK]\")\n",
    "        prediction = fill_mask_pipeline(masked_proverb)\n",
    "        for pred in prediction:\n",
    "            if pred['token_str'] == word:\n",
    "                predictions.append((word, pred['score']))\n",
    "                break\n",
    "    \n",
    "    # Sélection du mot avec le score le plus élevé\n",
    "    best_word = max(predictions, key=lambda x: x[1])[0]\n",
    "    \n",
    "    return best_word\n",
    "\n",
    "# Exemple d'utilisation de la fonction\n",
    "proverb_example = \"a beau [MASK] qui part de loin\"\n",
    "word_candidates = [\"vient\", \"revient\"]\n",
    "best_word = select_best_word(proverb_example, word_candidates)\n",
    "print(best_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Évaluation et Analyse des Résultats\n",
    "Évaluation des performances du système sur le jeu de test et analyse détaillée des erreurs et des succès."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour évaluer les performances du système sur le jeu de test\n",
    "def evaluate_system(test_data):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(test_data)\n",
    "    \n",
    "    for item in test_data:\n",
    "        masked_proverb = item['Masked']\n",
    "        word_list = item['Word_list']\n",
    "        correct_proverb = item['Proverb']\n",
    "        \n",
    "        # Détection des verbes dans le proverbe masqué\n",
    "        verbs = detect_verbs(masked_proverb)\n",
    "        \n",
    "        # Si des verbes sont détectés, remplacer le premier verbe détecté par [MASK]\n",
    "        if verbs:\n",
    "            masked_proverb = masked_proverb.replace(verbs[0], \"[MASK]\")\n",
    "        \n",
    "        # Sélection du meilleur mot parmi les candidats\n",
    "        best_word = select_best_word(masked_proverb, word_list)\n",
    "        \n",
    "        # Reconstruction du proverbe avec le meilleur mot sélectionné\n",
    "        corrected_proverb = masked_proverb.replace(\"[MASK]\", best_word)\n",
    "        \n",
    "        # Vérification si le proverbe corrigé est correct\n",
    "        if corrected_proverb == correct_proverb:\n",
    "            correct_predictions += 1\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "# Évaluation du système sur le jeu de test\n",
    "accuracy = evaluate_system(tests)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Analyse des résultats\n",
    "def analyze_results(test_data):\n",
    "    for item in test_data:\n",
    "        masked_proverb = item['Masked']\n",
    "        word_list = item['Word_list']\n",
    "        correct_proverb = item['Proverb']\n",
    "        \n",
    "        # Détection des verbes dans le proverbe masqué\n",
    "        verbs = detect_verbs(masked_proverb)\n",
    "        \n",
    "        # Si des verbes sont détectés, remplacer le premier verbe détecté par [MASK]\n",
    "        if verbs:\n",
    "            masked_proverb = masked_proverb.replace(verbs[0], \"[MASK]\")\n",
    "        \n",
    "        # Sélection du meilleur mot parmi les candidats\n",
    "        best_word = select_best_word(masked_proverb, word_list)\n",
    "        \n",
    "        # Reconstruction du proverbe avec le meilleur mot sélectionné\n",
    "        corrected_proverb = masked_proverb.replace(\"[MASK]\", best_word)\n",
    "        \n",
    "        # Affichage des résultats\n",
    "        print(f\"Masked Proverb: {masked_proverb}\")\n",
    "        print(f\"Word List: {word_list}\")\n",
    "        print(f\"Correct Proverb: {correct_proverb}\")\n",
    "        print(f\"Corrected Proverb: {corrected_proverb}\")\n",
    "        print(f\"Is Correct: {corrected_proverb == correct_proverb}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Analyse détaillée des résultats\n",
    "analyze_results(tests)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
