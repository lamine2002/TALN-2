{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charger les données\n",
    "Charger les données à partir du fichier JSON et les afficher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "def load_json_data(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        data = json.load(fp)\n",
    "    return data\n",
    "\n",
    "# Charger et afficher quelques exemples\n",
    "data = load_json_data('data/t2_qa_examples.json')\n",
    "print(\"Nombre total d'exemples:\", len(data))\n",
    "\n",
    "# utilisation de pprint pour afficher 5 exemples\n",
    "pprint(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définir les questions\n",
    "Définir les différentes questions pour les types when, where et event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les questions\n",
    "\n",
    "# Liste des questions\n",
    "questions = {\n",
    "    \"WHEN\": [\n",
    "        \"When did the incident occur?\",\n",
    "        \"What is the exact date and time when the incident occurred?\",\n",
    "        \"What time did the incident occur?\",\n",
    "        \"When did the incident take place?\",\n",
    "    ],\n",
    "    \"WHERE\": [  \n",
    "        \"Where did the event occur?\",\n",
    "        \"What is the exact location of the incident?\",\n",
    "        \"Where did the incident take place?\",\n",
    "        \"What is the location of the incident?\",\n",
    "    ],\n",
    "    \"EVENT\": [\n",
    "        \"What unfolded during the incident?\",\n",
    "        \"What happened during the incident?\",\n",
    "        \"What is the incident about?\",\n",
    "        \"Summarize in a few sentences what happened during the incident.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Afficher les questions\n",
    "for key, qs in questions.items():\n",
    "    print(f\"\\n{key} questions:\")\n",
    "    for q in qs:\n",
    "        print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charger le modèle de question-réponse\n",
    "Charger le modèle de question-réponse extractif de HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle de question-réponse\n",
    "from transformers import pipeline\n",
    "\n",
    "# Charger le modèle pré-entraîné de HuggingFace\n",
    "qa_model = pipeline('question-answering', model='bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Évaluer les questions\n",
    "Évaluer chaque question pour chaque type (when, where, event) et calculer les scores F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer les questions\n",
    "import numpy as np\n",
    "\n",
    "# Fonction pour évaluer les scores F1 pour une liste de questions\n",
    "def evaluate_questions(questions, data, key):\n",
    "    scores = []\n",
    "    for question in questions:\n",
    "        question_scores = []\n",
    "        for example in data:\n",
    "            context = example['text']\n",
    "            result = qa_model(question=question, context=context)\n",
    "            score = evaluate_f1(example[key], result['answer'])\n",
    "            question_scores.append(score)\n",
    "        scores.append(np.mean(question_scores))\n",
    "    return scores\n",
    "\n",
    "# Évaluer les questions de type WHEN\n",
    "when_scores = evaluate_questions(questions['WHEN'], data, 'WHEN')\n",
    "best_when_question = questions['WHEN'][np.argmax(when_scores)]\n",
    "print(f\"Meilleure question WHEN: {best_when_question} avec un score moyen de {max(when_scores):.3f}\")\n",
    "\n",
    "# Évaluer les questions de type WHERE\n",
    "where_scores = evaluate_questions(questions['WHERE'], data, 'WHERE')\n",
    "best_where_question = questions['WHERE'][np.argmax(where_scores)]\n",
    "print(f\"Meilleure question WHERE: {best_where_question} avec un score moyen de {max(where_scores):.3f}\")\n",
    "\n",
    "# Évaluer les questions de type EVENT\n",
    "event_scores = evaluate_questions(questions['EVENT'], data, 'EVENT')\n",
    "best_event_question = questions['EVENT'][np.argmax(event_scores)]\n",
    "print(f\"Meilleure question EVENT: {best_event_question} avec un score moyen de {max(event_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculer les moyennes des scores F1\n",
    "Calculer la moyenne des scores F1 pour chaque question de chaque type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les moyennes des scores F1\n",
    "\n",
    "# Fonction pour évaluer les scores F1 pour une liste de questions\n",
    "def evaluate_questions(questions, data, key):\n",
    "    scores = []\n",
    "    for question in questions:\n",
    "        question_scores = []\n",
    "        for example in data:\n",
    "            context = example['text']\n",
    "            result = qa_model(question=question, context=context)\n",
    "            score = evaluate_f1(example[key], result['answer'])\n",
    "            question_scores.append(score)\n",
    "        scores.append(np.mean(question_scores))\n",
    "    return scores\n",
    "\n",
    "# Évaluer les questions de type WHEN\n",
    "when_scores = evaluate_questions(questions['WHEN'], data, 'WHEN')\n",
    "best_when_question = questions['WHEN'][np.argmax(when_scores)]\n",
    "print(f\"Meilleure question WHEN: {best_when_question} avec un score moyen de {max(when_scores):.3f}\")\n",
    "\n",
    "# Évaluer les questions de type WHERE\n",
    "where_scores = evaluate_questions(questions['WHERE'], data, 'WHERE')\n",
    "best_where_question = questions['WHERE'][np.argmax(where_scores)]\n",
    "print(f\"Meilleure question WHERE: {best_where_question} avec un score moyen de {max(where_scores):.3f}\")\n",
    "\n",
    "# Évaluer les questions de type EVENT\n",
    "event_scores = evaluate_questions(questions['EVENT'], data, 'EVENT')\n",
    "best_event_question = questions['EVENT'][np.argmax(event_scores)]\n",
    "print(f\"Meilleure question EVENT: {best_event_question} avec un score moyen de {max(event_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sélectionner les meilleures questions\n",
    "Sélectionner la question avec la meilleure moyenne de score F1 pour chaque type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les meilleures questions\n",
    "\n",
    "# Fonction pour évaluer les scores F1 pour une liste de questions\n",
    "def evaluate_questions(questions, data, key):\n",
    "    scores = []\n",
    "    for question in questions:\n",
    "        question_scores = []\n",
    "        for example in data:\n",
    "            context = example['text']\n",
    "            result = qa_model(question=question, context=context)\n",
    "            score = evaluate_f1(example[key], result['answer'])\n",
    "            question_scores.append(score)\n",
    "        scores.append(np.mean(question_scores))\n",
    "    return scores\n",
    "\n",
    "# Évaluer les questions de type WHEN\n",
    "when_scores = evaluate_questions(questions['WHEN'], data, 'WHEN')\n",
    "best_when_question = questions['WHEN'][np.argmax(when_scores)]\n",
    "print(f\"Meilleure question WHEN: {best_when_question} avec un score moyen de {max(when_scores):.3f}\")\n",
    "\n",
    "# Évaluer les questions de type WHERE\n",
    "where_scores = evaluate_questions(questions['WHERE'], data, 'WHERE')\n",
    "best_where_question = questions['WHERE'][np.argmax(where_scores)]\n",
    "print(f\"Meilleure question WHERE: {best_where_question} avec un score moyen de {max(where_scores):.3f}\")\n",
    "\n",
    "# Évaluer les questions de type EVENT\n",
    "event_scores = evaluate_questions(questions['EVENT'], data, 'EVENT')\n",
    "best_event_question = questions['EVENT'][np.argmax(event_scores)]\n",
    "print(f\"Meilleure question EVENT: {best_event_question} avec un score moyen de {max(event_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Évaluer les meilleures questions\n",
    "Évaluer les meilleures questions sélectionnées sur l'ensemble des données et afficher les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer les meilleures questions\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Fonction pour évaluer les scores F1 pour une liste de questions\n",
    "def evaluate_questions(questions, data, key):\n",
    "    scores = []\n",
    "    for question in questions:\n",
    "        question_scores = []\n",
    "        for example in data:\n",
    "            context = example['text']\n",
    "            result = qa_model(question=question, context=context)\n",
    "            score = evaluate_f1(example[key], result['answer'])\n",
    "            question_scores.append(score)\n",
    "        scores.append(np.mean(question_scores))\n",
    "    return scores\n",
    "\n",
    "# Évaluer les questions de type WHEN\n",
    "when_scores = evaluate_questions(questions['WHEN'], data, 'WHEN')\n",
    "best_when_question = questions['WHEN'][np.argmax(when_scores)]\n",
    "print(f\"Meilleure question WHEN: {best_when_question} avec un score moyen de {max(when_scores):.3f}\")\n",
    "\n",
    "# Évaluer les questions de type WHERE\n",
    "where_scores = evaluate_questions(questions['WHERE'], data, 'WHERE')\n",
    "best_where_question = questions['WHERE'][np.argmax(where_scores)]\n",
    "print(f\"Meilleure question WHERE: {best_where_question} avec un score moyen de {max(where_scores):.3f}\")\n",
    "\n",
    "# Évaluer les questions de type EVENT\n",
    "event_scores = evaluate_questions(questions['EVENT'], data, 'EVENT')\n",
    "best_event_question = questions['EVENT'][np.argmax(event_scores)]\n",
    "print(f\"Meilleure question EVENT: {best_event_question} avec un score moyen de {max(event_scores):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
